
from langchain.agents import initialize_agent, Tool, AgentType
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.agents import initialize_agent, AgentType
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
from googlesearch import search
from dotenv import load_dotenv
import os
import streamlit as st

# Load the JSON data
laws_data = pd.read_json('Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†.json')

# Extract the "Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ©" column, which is a list of dictionaries
lawsNo_df = pd.json_normalize(laws_data["Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ©"])
faq_df = pd.read_excel('personal_status.xlsx')
# OpenAI API Key
# Load environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Initialize the LLM Model
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3, openai_api_key=OPENAI_API_KEY)

# Initialize Conversation Memory
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Arabic Law Prompt Template
arabic_prompt = """
Ø£Ù†Øª Ø±ÙˆØ¨ÙˆØª Ù…ØªØ®ØµØµ ÙÙŠ Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ©ØŒ ÙˆÙŠØ¬Ø¨ Ø£Ù† ØªØ±Ø¯ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§Øª ÙˆØ§Ø¶Ø­Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø© Ø­ÙˆÙ„ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ø²ÙˆØ§Ø¬ØŒ Ø§Ù„Ø·Ù„Ø§Ù‚ØŒ Ø§Ù„Ø­Ø¶Ø§Ù†Ø©ØŒ ÙˆØºÙŠØ±Ù‡Ø§.
ØªØ°ÙƒØ± Ù…Ø§ ØªÙ…Øª Ù…Ù†Ø§Ù‚Ø´ØªÙ‡ Ø³Ø§Ø¨Ù‚Ù‹Ø§ Ù„Ù…ÙˆØ§ØµÙ„Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨Ø³Ù„Ø§Ø³Ø©.
Chat History:
    {chat_history}
     Ø§Ø¬Ø¨ Ø¹Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø§ØªÙ‰ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŸ
Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ: {query}
"""

# Create Prompt Template
law_prompt_template = PromptTemplate(input_variables=["query",  "chat_history"],  template=arabic_prompt + " {query}")

# Initialize LLM Chain with Memory
llm_chain = LLMChain(llm=llm, prompt=law_prompt_template, memory=memory)

# Load Sentence Transformer Model
embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Function to Get Embeddings
def get_embedding(text: str):
    return embedding_model.encode(text)

# Function to Calculate Similarity for FAQs
def calculate_cosine_similarity(query, dataset, threshold=0.9):
    query_embedding = get_embedding(query)
    max_similarity, best_match = 0, None

    for _, row in dataset.iterrows():
        item_text = row.get('Ø³Ø¤Ø§Ù„', row.get('Ø§Ø¬Ø§Ø¨Ø©'))
        similarity = cosine_similarity([query_embedding], [get_embedding(item_text)])[0][0]

        if similarity > max_similarity and similarity > threshold:
            max_similarity, best_match = similarity, row

    return best_match
def calculate_cosine_similarity_rules(query, dataset, threshold=0.9):
    query_embedding = get_embedding(query)
    max_similarity, best_match = 0, None

    for _, row in dataset.iterrows():
        item_text = row['Ù†Øµ Ø§Ù„Ù…Ø§Ø¯Ø©'] if 'Ù†Øµ Ø§Ù„Ù…Ø§Ø¯Ø©' in row else row['Ù†Øµ Ø§Ù„Ù…Ø§Ø¯Ø©']
        similarity = cosine_similarity([query_embedding], [get_embedding(item_text)])[0][0]

        if similarity > max_similarity and similarity > threshold:
            max_similarity, best_match = similarity, row

    return best_match
# Function to Search in FAQs
def search_faq(query):
    result = calculate_cosine_similarity(query, faq_df)
    return result['Ø§Ø¬Ø§Ø¨Ø©'] if result is not None else None

def search_rule(query):
    result = calculate_cosine_similarity_rules(query, lawsNo_df)
    return (result['Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø©'], result['Ù†Øµ Ø§Ù„Ù…Ø§Ø¯Ø©']) if result is not None else None

# Function to Search Google (Fallback)
def search_google(query):
    try:
        results = list(search(query, num_results=5))  # Convert generator to a list
        print(f"Search Results: {results}")  # Debugging output
        if not results:
            return "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø©."
        return results[0]  # Return the first result
    except Exception as e:
        print(f"Error during Google Search: {e}")  # Print error if any
        return "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¬ÙˆØ¬Ù„."
# Define Tools for LangChain Agent
faq_tool = Tool(name="FAQ Search", func=search_faq, description="Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©.")
rule_tool = Tool(name="Rule Search", func=search_rule, description="Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ†.")
google_tool = Tool(name="Google Search", func=search_google, description="Ø§Ù„Ø¨Ø­Ø« Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª .")

# Initialize LangChain Agent with Memory
agent = initialize_agent(
    tools=[faq_tool, rule_tool, google_tool],
    llm=llm,
    agent_type=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
    memory=memory, 
    agent_kwargs={"prompt": law_prompt_template} , 
    verbose=True
)


def chatbot(query):
    # First, try FAQ answers
    faq_answer = search_faq(query)
    if faq_answer:
        refined_answer = llm_chain.run(f"Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„: {query}\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {faq_answer}")
        return refined_answer

    # If FAQ answer is not found, try searching in the rules
    rule_answer = search_rule(query)
    if rule_answer:
        refined_answer = f"ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ù…Ø§Ø¯Ø© {rule_answer[0]}: {rule_answer[1]}"
        refined_answer = llm_chain.run(f"Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„: {query}\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {refined_answer}")
        return refined_answer

    # If no rule answer, try Google Search
    google_answer = search_google(query)
    if google_answer:
        refined_answer = llm_chain.run(f"Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„: {query}\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {google_answer}")
        return refined_answer

    # If no answer found, fallback to LangChain agent
    return agent.run(query)    




# Streamlit UI
st.title("ğŸ¤– Arabic Legal Chatbot")
st.write("Chat with an AI expert on **Egyptian Personal Status Law** ğŸ‡ªğŸ‡¬")

# Manage session state
if "chat_active" not in st.session_state:
    st.session_state.chat_active = True


# Only show input if chat is active
if st.session_state.chat_active:
    user_input = st.text_input("Ø£Ø¯Ø®Ù„ Ø³Ø¤Ø§Ù„Ùƒ", "")
    
    if st.button("Ø§Ù„Ø§Ø¬Ø§Ø¨Ø©"):
        response = chatbot(user_input)
        
        if not isinstance(response, str):
            response = str(response)  # Ensure response is a string
            print("done")
        st.write("**Ù…Ø³Ø§Ø¹Ø¯Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:**", response)

# End chat button
if st.button("Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
    st.session_state.chat_active = False
    st.write("ØªÙ… Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.")
